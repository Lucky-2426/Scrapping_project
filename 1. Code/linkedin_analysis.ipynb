{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- LinkedIn Analyzer Notebook ----------\n",
    "\n",
    "This notebook will help to analyze the job offers scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import langcodes\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobOffer():\n",
    "    def __init__(self, job_description, language, offer_ID):\n",
    "        self.description = job_description\n",
    "        self.offer_ID = offer_ID\n",
    "        self.language = language\n",
    "        self._analyze()\n",
    "        self._finalize_report()\n",
    "\n",
    "    def _analyze(self):\n",
    "        self.df_analysis = pd.DataFrame(columns = ['Word', 'Count', 'Category'])\n",
    "\n",
    "        # Lowercase the characters\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", self.description.lower())\n",
    "        words = text.split()\n",
    "\n",
    "        # Because stopwords is dependant of the language, we should use the language as a parameter\n",
    "        words = [w for w in words if w not in stopwords.words(self.language)]\n",
    "        freq_words = nltk.FreqDist(words)\n",
    "        for key in freq_words.keys():\n",
    "            self.df_analysis = self.df_analysis.append({'Word' : key, 'Count': freq_words[key], 'Category': 'word'}, ignore_index = True)\n",
    "\n",
    "        # Reduce words to their stem\n",
    "        stemmed = [SnowballStemmer(language = self.language).stem(w) for w in words]\n",
    "        freq_stemmed = nltk.FreqDist(stemmed)\n",
    "        for key in freq_stemmed.keys():\n",
    "            self.df_analysis = self.df_analysis.append({'Word' : key, 'Count': freq_stemmed[key], 'Category': 'stem'}, ignore_index = True)\n",
    "        \n",
    "        # Reduce words to their lem\n",
    "        lemmed  = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "        freq_lemmed = nltk.FreqDist(lemmed)\n",
    "        for key in freq_lemmed.keys():\n",
    "            self.df_analysis = self.df_analysis.append({'Word' : key, 'Count': freq_lemmed[key], 'Category': 'lem'}, ignore_index = True)\n",
    "        \n",
    "        finder = nltk.collocations.BigramCollocationFinder.from_words(lemmed)\n",
    "        for key in finder.ngram_fd.keys():\n",
    "            new_key = key[0] + ' ' + key[1]\n",
    "            self.df_analysis = self.df_analysis.append({'Word' : new_key, 'Count': finder.ngram_fd[key], 'Category': 'bigram'}, ignore_index = True)\n",
    "        # dfBigrams['bigram'] = finder.ngram_fd.keys()\n",
    "        # dfBigrams['count']  = finder.ngram_fd.values()\n",
    "\n",
    "    def _finalize_report(self):\n",
    "        self.df_analysis['offerID']  = self.offer_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolOffers():\n",
    "    def __init__(self, filename):\n",
    "        self.df      = pd.read_excel(filename)\n",
    "        self._generate_pool()\n",
    "\n",
    "    def _generate_pool(self):\n",
    "        self.table_analysis = []\n",
    "\n",
    "        self.df['Language'] = self.df.apply(lambda row: self._find_offer_language(row['Job_Description']), axis = 1)\n",
    "        self.df['Estimated_post_date'] = self.df.apply(lambda row: self._calculate_date(row['Posted_Date']), axis = 1)\n",
    "        self.df.drop(columns = 'Posted_Date', inplace = True)\n",
    "\n",
    "        for index, row in tqdm(self.df.iterrows()):\n",
    "            self.table_analysis.append(JobOffer(row['Job_Description'], row['Language'], index))\n",
    "\n",
    "    def _calculate_date(self, text_date):\n",
    "        if 'month' in text_date:\n",
    "            final_date = date.today() - relativedelta(months  = int(text_date.split(' ')[0]))\n",
    "        elif 'week' in text_date:\n",
    "            final_date = date.today() - relativedelta(days    = int(text_date.split(' ')[0])*7)\n",
    "        elif 'day' in text_date:\n",
    "            final_date = date.today() - relativedelta(days    = int(text_date.split(' ')[0]))\n",
    "        elif 'hour' in text_date:\n",
    "            final_date = date.today() - relativedelta(hours   = int(text_date.split(' ')[0]))\n",
    "        elif ('min' and 'ago') in text_date:\n",
    "            final_date = date.today() - relativedelta(minutes = int(text_date.split(' ')[0]))\n",
    "        else:\n",
    "            final_date = 'Error'\n",
    "        \n",
    "        return final_date\n",
    "\n",
    "    def _find_offer_language(self, description):\n",
    "        # Identify the language\n",
    "        \n",
    "        DetectorFactory.seed = 0\n",
    "        self.language = langcodes.Language.make(language = detect(description)).display_name().lower()\n",
    "        if self.language == \"afrikaans\" or self.language == \"german\":\n",
    "            self.language = \"dutch\"\n",
    "        elif self.language == \"catalan\":\n",
    "            self.language = \"french\"\n",
    "\n",
    "        return self.language\n",
    "\n",
    "    def analyze(self):\n",
    "        self.report = pd.DataFrame()\n",
    "\n",
    "        for offer in self.table_analysis:\n",
    "            self.report = pd.concat([self.report, offer.df_analysis], axis = 0)\n",
    "\n",
    "        return self.df, self.report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\lkakpo\\Desktop\\MUSE\\1. Code\\linkedin_analysis.ipynb Cellule 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m file \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m2. Exports\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m1. Export scraper\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mteeest.xlsx\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m myPool \u001b[39m=\u001b[39m PoolOffers(file)\n",
      "\u001b[1;32mc:\\lkakpo\\Desktop\\MUSE\\1. Code\\linkedin_analysis.ipynb Cellule 5\u001b[0m in \u001b[0;36mPoolOffers.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, filename):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf      \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_excel(filename)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_pool()\n",
      "\u001b[1;32mc:\\lkakpo\\Desktop\\MUSE\\1. Code\\linkedin_analysis.ipynb Cellule 5\u001b[0m in \u001b[0;36mPoolOffers._generate_pool\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_pool\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtable_analysis \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mLanguage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m row: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_offer_language(row[\u001b[39m'\u001b[39;49m\u001b[39mJob_Description\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mEstimated_post_date\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_date(row[\u001b[39m'\u001b[39m\u001b[39mPosted_Date\u001b[39m\u001b[39m'\u001b[39m]), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mdrop(columns \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPosted_Date\u001b[39m\u001b[39m'\u001b[39m, inplace \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\lkakpo.BEIJAFLORE\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:8839\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   8828\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[0;32m   8830\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[0;32m   8831\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   8832\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   8837\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   8838\u001b[0m )\n\u001b[1;32m-> 8839\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lkakpo.BEIJAFLORE\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:727\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[0;32m    725\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[1;32m--> 727\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\lkakpo.BEIJAFLORE\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:851\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[0;32m    853\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\lkakpo.BEIJAFLORE\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:867\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    865\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[0;32m    866\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 867\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[0;32m    868\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    869\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    870\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    871\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\lkakpo\\Desktop\\MUSE\\1. Code\\linkedin_analysis.ipynb Cellule 5\u001b[0m in \u001b[0;36mPoolOffers._generate_pool.<locals>.<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_pool\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtable_analysis \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mLanguage\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_offer_language(row[\u001b[39m'\u001b[39;49m\u001b[39mJob_Description\u001b[39;49m\u001b[39m'\u001b[39;49m]), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mEstimated_post_date\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m row: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calculate_date(row[\u001b[39m'\u001b[39m\u001b[39mPosted_Date\u001b[39m\u001b[39m'\u001b[39m]), axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mdrop(columns \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPosted_Date\u001b[39m\u001b[39m'\u001b[39m, inplace \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32mc:\\lkakpo\\Desktop\\MUSE\\1. Code\\linkedin_analysis.ipynb Cellule 5\u001b[0m in \u001b[0;36mPoolOffers._find_offer_language\u001b[1;34m(self, description)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_find_offer_language\u001b[39m(\u001b[39mself\u001b[39m, description):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Identify the language\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     DetectorFactory\u001b[39m.\u001b[39mseed \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage \u001b[39m=\u001b[39m langcodes\u001b[39m.\u001b[39mLanguage\u001b[39m.\u001b[39mmake(language \u001b[39m=\u001b[39m detect(description))\u001b[39m.\u001b[39mdisplay_name()\u001b[39m.\u001b[39mlower()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mafrikaans\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgerman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/lkakpo/Desktop/MUSE/1.%20Code/linkedin_analysis.ipynb#W4sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlanguage \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdutch\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\lkakpo.BEIJAFLORE\\Anaconda3\\lib\\site-packages\\langdetect\\detector_factory.py:129\u001b[0m, in \u001b[0;36mdetect\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    127\u001b[0m init_factory()\n\u001b[0;32m    128\u001b[0m detector \u001b[39m=\u001b[39m _factory\u001b[39m.\u001b[39mcreate()\n\u001b[1;32m--> 129\u001b[0m detector\u001b[39m.\u001b[39;49mappend(text)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m detector\u001b[39m.\u001b[39mdetect()\n",
      "File \u001b[1;32mc:\\Users\\lkakpo.BEIJAFLORE\\Anaconda3\\lib\\site-packages\\langdetect\\detector.py:104\u001b[0m, in \u001b[0;36mDetector.append\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mappend\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m    100\u001b[0m     \u001b[39m'''Append the target text for language detection.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m    If the total size of target text exceeds the limit size specified by\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m    Detector.set_max_text_length(int), the rest is cut down.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mURL_RE\u001b[39m.\u001b[39;49msub(\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m, text)\n\u001b[0;32m    105\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMAIL_RE\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, text)\n\u001b[0;32m    106\u001b[0m     text \u001b[39m=\u001b[39m NGram\u001b[39m.\u001b[39mnormalize_vi(text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "file = os.path.join('..', '2. Exports', '1. Export scraper', 'teeest.xlsx')\n",
    "myPool = PoolOffers(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_offers, table_analysis = myPool.analyze()\n",
    "print(table_offers.head(2))\n",
    "print(table_analysis.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_analysis.to_excel(os.path.join('..', '2. Exports', 'table_analysis.xlsx'))\n",
    "table_offers.to_excel(os.path.join('..', '2. Exports', 'table_offers.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_offers = pd.read_excel(\"All search data consultant.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_offers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = df_job_offers[\"Job_Description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus = df_corpus.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_offers = []\n",
    "nl_offers = []\n",
    "\n",
    "for text in list_corpus:\n",
    "    if \"Je bent\" in text or \"Belgische\" in text or \"Kennis\" in text or \"jij\" in text:\n",
    "        nl_offers.append(text)\n",
    "    else:\n",
    "        en_offers.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of offers in Dutch:\", len(nl_offers))\n",
    "print(\"Number of offers in English:\", len(en_offers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for off in nl_offers[:10]:\n",
    "    print(off[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary allows me to put the language in front of the list for the following steps that depends of the languages\n",
    "total_offers = {'english': en_offers, 'dutch': nl_offers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "stemmed = {}\n",
    "lemmed = {}\n",
    "\n",
    "for offer in total_offers:\n",
    "\n",
    "    texts = \"\"\n",
    "\n",
    "    for i, _ in enumerate(total_offers[offer]):\n",
    "        text = total_offers[offer][i]\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "        texts = texts + \" \" + text\n",
    "        # print(words[:4])\n",
    "\n",
    "        # Because stopwords is dependant of the language, I use the key of the dictionary\n",
    "    \n",
    "    words[offer] = texts.split()\n",
    "    words[offer] = [w for w in words[offer] if w not in stopwords.words(offer)]\n",
    "\n",
    "    # Reduce words to their stems\n",
    "    stemmed[offer] = [SnowballStemmer(language=offer).stem(w) for w in words[offer]]\n",
    "    \n",
    "    # Reduce words to their root form\n",
    "    lemmed[offer]  = [WordNetLemmatizer().lemmatize(w) for w in words[offer]]\n",
    "\n",
    "    print(words[offer][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words['dutch'][:15])\n",
    "print(words['english'][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Analysis\n",
    "\n",
    "Analyzing the frequencies of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in words:\n",
    "    columns = ['lemmed_words', 'lemmed_count', 'stemmed_words', 'stemmed_count', 'words', 'words_count']\n",
    "    df = pd.DataFrame(columns = columns)\n",
    "\n",
    "    freq = nltk.FreqDist(words[language])\n",
    "    df['words'] = freq.keys()\n",
    "    df['words_count'] = freq.values()\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    freq = nltk.FreqDist(lemmed[language])\n",
    "    df['lemmed_words'] = pd.Series(freq.keys())\n",
    "    df['lemmed_count'] = pd.Series(freq.values())\n",
    "\n",
    "    freq = nltk.FreqDist(stemmed[language])\n",
    "    df['stemmed_words'] = pd.Series(freq.keys())\n",
    "    df['stemmed_count'] = pd.Series(freq.values())\n",
    "\n",
    "\n",
    "    columns = ['bigram', 'count']\n",
    "    dfBigrams = pd.DataFrame(columns = columns)\n",
    "    \n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(words[language])\n",
    "    dfBigrams['bigram'] = finder.ngram_fd.keys()\n",
    "    dfBigrams['count']  = finder.ngram_fd.values()\n",
    "\n",
    "    df.to_excel(\"df_\" + language + \".xlsx\")\n",
    "    dfBigrams.to_excel(\"dfBigrams_\" + language + \".xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finder = nltk.collocations.BigramCollocationFinder.from_words(words['english'])\n",
    "finder.ngram_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3af129071f818f4dfea968a0287e88a179fda60faee344877b27967a8c6a3b60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
